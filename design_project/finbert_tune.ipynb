{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Chan Wei Jian Ivan\n",
    "\n",
    "ID: 1005924"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our project, we will be pulling Bert-base-uncased from hugging face and finetuning it for predicting movie sentiment reviews using the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load the IMDB dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Split the training set into 80% training and 20% validation\n",
    "train_testvalid = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Further split the validation set into 50% validation and 50% test\n",
    "valid_test = train_testvalid[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# Combine splits into a DatasetDict\n",
    "dataset_split = DatasetDict({\n",
    "    \"train\": train_testvalid[\"train\"],\n",
    "    \"validation\": train_testvalid[\"test\"],\n",
    "    \"test\": dataset[\"test\"]  # Use the original test set as-is\n",
    "})\n",
    "\n",
    "# Print the sizes of the splits\n",
    "print(f\"Train set: {len(dataset_split['train'])} rows\")\n",
    "print(f\"Validation set: {len(dataset_split['validation'])} rows\")\n",
    "print(f\"Test set: {len(dataset_split['test'])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24dd692383d45a083a980e98a94df8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d2db7ddc634c45a4b245751c6debc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7805034e56748259349281948f1660a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0585e0ca8a35401c8fef3f8938248e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d32a7c3020b40f1a3e5248136785af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b1fe3da8e645f284ed6178b69b2088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c90e5e7d56489583f776d1106da236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e83e47e3b384387a313b0fc2f6da166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_input_length = tokenizer.model_max_length\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT's maximum input length is 512 tokens, however based on the histogram done in the project notebook, most of the reviews are around 250 tokens. Hence it is sufficient to truncate the reviews to 256 for predicting sentiment while reducing the amount of computation required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "## Apply preprocessing\n",
    "tokenized_dataset = dataset_split.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class IMDBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        # Extract input encodings and labels from the Dataset object\n",
    "        self.encodings = {\n",
    "            \"input_ids\": dataset[\"input_ids\"],\n",
    "            \"attention_mask\": dataset[\"attention_mask\"]\n",
    "        }\n",
    "        self.labels = dataset[\"label\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Access encodings and labels for the given index\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(tokenized_dataset[\"train\"])\n",
    "validation_dataset = IMDBDataset(tokenized_dataset[\"validation\"])\n",
    "test_dataset = IMDBDataset(tokenized_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenized dataset\n",
    "import torch\n",
    "\n",
    "# Save the datasets\n",
    "torch.save(train_dataset, \"train_dataset.pt\")\n",
    "torch.save(validation_dataset, \"validation_dataset.pt\")\n",
    "torch.save(test_dataset, \"test_dataset.pt\")\n",
    "\n",
    "print(\"Datasets saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load BERT from transformers\n",
    "\n",
    "We add a classification head with a dropout layer to reduce overfitting and a fully connected layer for sentiment prediction.\n",
    "\n",
    "Lastly, we extract the CLS token for classification. The CLS token is used to represent the sentence level classification. It contains the embedding that summarizes the sentiment of the entire sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, num_labels):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        # Load pretrained BERT model\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_model_name)\n",
    "\n",
    "        # Define a dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Define a linear layer for classification\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        # Pass inputs through BERT\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=False\n",
    "        )\n",
    "\n",
    "        # Use the [CLS] token output\n",
    "        pooled_output = outputs[1]\n",
    "        # Apply dropout\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        # Apply classification layer\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Initialize the model\n",
    "model = SentimentClassifier(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ivanc\\AppData\\Local\\Temp\\ipykernel_25160\\3962022104.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_dataset = torch.load(\"train_dataset.pt\")\n",
      "C:\\Users\\ivanc\\AppData\\Local\\Temp\\ipykernel_25160\\3962022104.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  validation_dataset = torch.load(\"validation_dataset.pt\")\n",
      "C:\\Users\\ivanc\\AppData\\Local\\Temp\\ipykernel_25160\\3962022104.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_dataset = torch.load(\"test_dataset.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "train_dataset = torch.load(\"train_dataset.pt\")\n",
    "validation_dataset = torch.load(\"validation_dataset.pt\")\n",
    "test_dataset = torch.load(\"test_dataset.pt\")\n",
    "\n",
    "print(\"Datasets loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# DataLoaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Average Training Loss: 0.1702\n",
      "Validation Accuracy: 0.9060\n",
      "Model saved at ./sentiment_model_epoch_1.pt\n",
      "Epoch 2/3\n",
      "Average Training Loss: 0.0981\n",
      "Validation Accuracy: 0.9038\n",
      "Model saved at ./sentiment_model_epoch_2.pt\n",
      "Epoch 3/3\n",
      "Average Training Loss: 0.0642\n",
      "Validation Accuracy: 0.9022\n",
      "Model saved at ./sentiment_model_epoch_3.pt\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "epochs = 3  # Number of training epochs\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    # Training loop\n",
    "    for batch in train_loader:\n",
    "        # Move data to the device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(input_ids, attention_mask)  # Model now directly returns logits\n",
    "        loss = criterion(logits, labels)  # Compute loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update parameters\n",
    "\n",
    "    # Log average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Average Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation step after each epoch\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No gradient computation for validation\n",
    "        for batch in validation_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            predictions = torch.argmax(logits, dim=1)  # Get class predictions\n",
    "\n",
    "            # Calculate accuracy\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    # Compute validation accuracy\n",
    "    accuracy = correct / total\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Save the model after each epoch\n",
    "\n",
    "\n",
    "    save_path = f\"./sentiment_model_epoch_{epoch + 1}.pt\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Model saved at {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9002\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Evaluation\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Disable gradient computation\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move data to the device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        predictions = torch.argmax(logits, dim=1)  # Get predicted class labels\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "# Compute final accuracy\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ivanc\\AppData\\Local\\Temp\\ipykernel_25160\\802716415.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  current_model.load_state_dict(torch.load(\"sentiment_model_epoch_3.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "current_model = SentimentClassifier(\"bert-base-uncased\", num_labels=2)\n",
    "current_model.load_state_dict(torch.load(\"sentiment_model_epoch_3.pt\"))\n",
    "current_model.to(device)\n",
    "\n",
    "# Define the new sequence\n",
    "sequence = \"\"\"Julie Andrews satirically prods her own goody-two-shoes image in this overproduced musical comedy-drama, but if she approaches her role with aplomb, she's alone in doing so. Blake Edwards' film about a woman who is both music-hall entertainer and German spy during WWI doesn't know what tone to aim for, and Rock Hudson has the thankless task of playing romantic second-fiddle. Musicals had grown out of favor by 1970, and elephantine productions like \"Star!\" and this film really tarnished Andrews' reputation, leaving a lot of dead space in her catalogue until \"The Tamarind Seed\" came along. I've always thought Julie Andrews would've made a great villain or shady lady; her strong voice could really command attention, and she hits some low notes that can either be imposing or seductive. Husband/director Edwards seems to realize this, but neither he nor Julie can work up much energy within this scenario. Screenwriter William Peter Blatty isn't a good partner for Edwards, and neither man has his heart in this material. Beatty's script offers Andrews just one fabulous sequence--a striptease. *1/2 from ****\"\"\"\n",
    "\n",
    "# Preprocess the sequence\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "# Move tensors to the device\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "# Run the model in evaluation mode\n",
    "current_model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = current_model(input_ids, attention_mask)\n",
    "    predictions = torch.argmax(logits, dim=1)  # Get the predicted class (0 or 1)\n",
    "\n",
    "# Decode the prediction\n",
    "label_map = {0: \"Negative\", 1: \"Positive\"}  # Adjust based on your dataset's labels\n",
    "predicted_label = label_map[predictions.item()]\n",
    "print(f\"Predicted Sentiment: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
